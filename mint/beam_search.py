#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
MINT TextGraph - Beam Search Path Finding
TÃ¬m Ä‘Æ°á»ng Ä‘i tá»« claim Ä‘áº¿n sentence nodes báº±ng Beam Search
"""

import json
import os
from collections import defaultdict
from typing import List, Dict, Tuple, Set, Optional
import heapq
from datetime import datetime
import time
from difflib import SequenceMatcher
import networkx as nx


class Path:
    """Äáº¡i diá»‡n cho má»™t Ä‘Æ°á»ng Ä‘i trong Ä‘á»“ thá»‹"""
    
    def __init__(self, nodes: List[str], edges: Optional[List[Tuple[str, str, str]]] = None, score: float = 0.0):
        self.nodes = nodes  # Danh sÃ¡ch node IDs
        self.edges = edges or []  # Danh sÃ¡ch (from_node, to_node, relation)
        self.score = score  # Äiá»ƒm Ä‘Ã¡nh giÃ¡ path
        self.claim_words = set()  # Words trong claim Ä‘á»ƒ so sÃ¡nh
        self.word_matches = set()  # âœ… THÃŠM: Set of matched words
        self.path_words = set()   # Tá»« trong path
        self.entities_visited = set()  # Entities Ä‘Ã£ Ä‘i qua
        
    def __lt__(self, other):
        """So sÃ¡nh Ä‘á»ƒ sort paths theo score"""
        return self.score < other.score
        
    def add_node(self, node_id: str, edge_info: Optional[Tuple[str, str, str]] = None):
        """ThÃªm node vÃ o path"""
        self.nodes.append(node_id)
        if edge_info:
            self.edges.append(edge_info)
            
    def copy(self):
        """Táº¡o báº£n copy cá»§a path"""
        new_path = Path(self.nodes.copy(), self.edges.copy(), self.score)
        new_path.claim_words = self.claim_words.copy()
        new_path.word_matches = self.word_matches.copy()
        new_path.path_words = self.path_words.copy()
        new_path.entities_visited = self.entities_visited.copy()
        return new_path
        
    def get_current_node(self):
        """Láº¥y node hiá»‡n táº¡i (cuá»‘i path)"""
        return self.nodes[-1] if self.nodes else None
        
    def contains_node(self, node_id: str):
        """Kiá»ƒm tra path cÃ³ chá»©a node nÃ y khÃ´ng"""
        return node_id in self.nodes
        
    def to_dict(self):
        """Convert path thÃ nh dictionary Ä‘á»ƒ export"""
        return {
            'nodes': self.nodes,
            'edges': self.edges,
            'score': self.score,
            'length': len(self.nodes),
            'claim_words_matched': len(self.claim_words.intersection(self.path_words)),
            'total_claim_words': len(self.claim_words),
            'entities_visited': list(self.entities_visited),
            'path_summary': self._get_path_summary()
        }
        
    def _get_path_summary(self):
        """Táº¡o summary ngáº¯n gá»n cá»§a path"""
        node_types = []
        for node in self.nodes:
            if node.startswith('claim'):
                node_types.append('CLAIM')
            elif node.startswith('word'):
                node_types.append('WORD')
            elif node.startswith('sentence'):
                node_types.append('SENTENCE')
            elif node.startswith('entity'):
                node_types.append('ENTITY')
            else:
                node_types.append('UNKNOWN')
        return ' -> '.join(node_types)


class BeamSearchPathFinder:
    """Beam Search Ä‘á»ƒ tÃ¬m Ä‘Æ°á»ng Ä‘i tá»« claim Ä‘áº¿n sentence nodes"""
    
    def __init__(self, text_graph, beam_width: int = 25, max_depth: int = 30, allow_skip_edge: bool = False):
        self.graph = text_graph
        self.beam_width = beam_width
        self.max_depth = max_depth
        # Cho phÃ©p "nháº£y" qua má»™t nÃºt trung gian (2-hop) náº¿u cáº§n má»Ÿ rá»™ng Ä‘a dáº¡ng
        self.allow_skip_edge = allow_skip_edge
        self.claim_words = set()  # Words trong claim
        
        # Scoring weights - âœ… Cáº¢I THIá»†N WEIGHTS
        self.word_match_weight = 5.0        # TÄƒng tá»« 3.0 lÃªn 5.0
        self.semantic_match_weight = 3.0    # âœ… Má»šI: Semantic similarity
        self.entity_bonus = 2.5             # TÄƒng tá»« 2.0 lÃªn 2.5
        self.length_penalty = 0.05          # Giáº£m tá»« 0.1 xuá»‘ng 0.05
        self.sentence_bonus = 4.0           
        self.fuzzy_match_weight = 2.0       # âœ… Má»šI: Fuzzy string matching
        
        # Stats
        self.paths_explored = 0
        self.sentence_paths_found = 0
        
        # New flag
        self.early_stop_on_sentence = True
        
    def extract_claim_words(self):
        """TrÃ­ch xuáº¥t táº¥t cáº£ tá»« trong claim Ä‘á»ƒ so sÃ¡nh"""
        claim_words = set()
        
        if self.graph.claim_node:
            # Láº¥y táº¥t cáº£ word nodes connected Ä‘áº¿n claim
            for neighbor in self.graph.graph.neighbors(self.graph.claim_node):
                node_data = self.graph.graph.nodes[neighbor]
                if node_data.get('type') == 'word':
                    claim_words.add(node_data.get('text', '').lower())
                    
        self.claim_words = claim_words
        return claim_words
        
    def _calculate_semantic_similarity(self, claim_words, path_words):
        """
        âœ… Má»šI: TÃ­nh semantic similarity giá»¯a claim vÃ  path words
        Sá»­ dá»¥ng Jaccard similarity vÃ  word overlap
        """
        if not claim_words or not path_words:
            return 0.0
            
        # Jaccard similarity
        intersection = claim_words.intersection(path_words)
        union = claim_words.union(path_words)
        jaccard = len(intersection) / len(union) if union else 0.0
        
        # Word overlap ratio
        overlap_ratio = len(intersection) / len(claim_words) if claim_words else 0.0
        
        # Combine scores
        semantic_score = (jaccard * 0.4) + (overlap_ratio * 0.6)
        return semantic_score
        
    def _calculate_fuzzy_similarity(self, claim_text, sentence_text):
        """
        âœ… Má»šI: TÃ­nh fuzzy string similarity
        """
        if not claim_text or not sentence_text:
            return 0.0
            
        # Normalize texts
        claim_normalized = claim_text.lower().strip()
        sentence_normalized = sentence_text.lower().strip()
        
        # Calculate similarity
        similarity = SequenceMatcher(None, claim_normalized, sentence_normalized).ratio()
        return similarity
        
    def score_path(self, path: Path) -> float:
        """âœ… Cáº¢I THIá»†N: TÃ­nh Ä‘iá»ƒm cho má»™t path vá»›i nhiá»u metrics hÆ¡n"""
        
        if not path.nodes:
            return 0.0
            
        # Láº¥y claim text Ä‘á»ƒ so sÃ¡nh
        claim_text = ""
        claim_words = set()
        
        for node in path.nodes:
            node_data = self.graph.graph.nodes[node]
            if node_data.get('type') == 'claim':
                claim_text = node_data.get('text', '')
                claim_words = set(claim_text.lower().split())
                break
                
        # Base score
        score = 0.0
        
        # 1. âœ… Cáº¢I THIá»†N: Enhanced Word matching score
        path_words = set()
        sentence_texts = []
        
        for node in path.nodes:
            node_data = self.graph.graph.nodes[node]
            node_text = node_data.get('text', '')
            node_type = node_data.get('type', '')
            
            if node_text:
                path_words.update(node_text.lower().split())
                
                # Collect sentence texts for fuzzy matching
                if node_type == 'sentence':
                    sentence_texts.append(node_text)
                
        if claim_words:
            word_matches = claim_words.intersection(path_words)
            word_match_ratio = len(word_matches) / len(claim_words)
            score += word_match_ratio * self.word_match_weight
            path.word_matches = word_matches
            
            # 2. âœ… Má»šI: Semantic similarity
            semantic_score = self._calculate_semantic_similarity(claim_words, path_words)
            score += semantic_score * self.semantic_match_weight
            
        # 3. âœ… Má»šI: Fuzzy matching vá»›i sentences
        if claim_text and sentence_texts:
            max_fuzzy_score = 0.0
            for sentence_text in sentence_texts:
                fuzzy_score = self._calculate_fuzzy_similarity(claim_text, sentence_text)
                max_fuzzy_score = max(max_fuzzy_score, fuzzy_score)
            score += max_fuzzy_score * self.fuzzy_match_weight
            
        # 4. âœ… ENHANCED: Dual-source entity scoring vá»›i weighted bonus
        entity_bonus_total = 0.0
        dual_source_entities = 0
        single_source_entities = 0
        
        for node in path.nodes:
            node_data = self.graph.graph.nodes[node]
            if node_data.get('type') == 'entity':
                # Get entity score (dual-source entities have score â‰¥ 2.0)
                entity_score = node_data.get('entity_score', 1.0)
                is_dual_source = node_data.get('is_dual_source', False)
                
                # All entities get equal treatment now
                entity_bonus_total += entity_score * self.entity_bonus
                if is_dual_source:
                    dual_source_entities += 1
                else:
                    single_source_entities += 1
                
        score += entity_bonus_total
        
        # Store entity path metadata for debugging
        path.dual_source_count = dual_source_entities
        path.single_source_count = single_source_entities
        # Keep entities_visited as set - don't overwrite with int
        
        # 5. âœ… Cáº¢I THIá»†N: Giáº£m length penalty
        score -= len(path.nodes) * self.length_penalty
        
        # 6. âœ… THÃŠM: Sentence relevance bonus
        sentence_count = sum(1 for node in path.nodes 
                           if self.graph.graph.nodes[node].get('type') == 'sentence')
        if sentence_count > 0:
            score += sentence_count * 1.5  # Bonus cho má»—i sentence trong path
            
        return score
        
    def beam_search(self, start_node: str = None) -> List[Path]:
        """
        Thá»±c hiá»‡n Beam Search tá»« claim node Ä‘áº¿n sentence nodes
        
        Returns:
            List[Path]: Danh sÃ¡ch cÃ¡c paths tá»‘t nháº¥t tÃ¬m Ä‘Æ°á»£c
        """
        if start_node is None:
            start_node = self.graph.claim_node
            
        if not start_node:
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y claim node Ä‘á»ƒ báº¯t Ä‘áº§u beam search")
            return []
            
        # Extract claim words Ä‘á»ƒ scoring
        self.extract_claim_words()
        
        # Prepare graph data for faster lookup
        graph_data = dict(self.graph.graph.nodes(data=True))
        
        # Initialize beam vá»›i path tá»« claim node
        beam = [Path([start_node])]
        completed_paths = []  # Paths Ä‘Ã£ Ä‘áº¿n sentence nodes
        
        print(f"ğŸ¯ Starting Beam Search from {start_node}")
        print(f"ğŸ“Š Beam width: {self.beam_width}, Max depth: {self.max_depth}")
        print(f"ğŸ’­ Claim words: {self.claim_words}")
        
        for depth in range(self.max_depth):
            if not beam:
                break
                
            print(f"\nğŸ” Depth {depth + 1}/{self.max_depth} - Current beam size: {len(beam)}")
            
            new_candidates = []
            
            # Expand má»—i path trong beam hiá»‡n táº¡i
            for path in beam:
                current_node = path.get_current_node()
                
                # Láº¥y táº¥t cáº£ neighbors cá»§a current node
                neighbors = list(self.graph.graph.neighbors(current_node))
                
                for neighbor in neighbors:
                    # TrÃ¡nh cycle - khÃ´ng quay láº¡i node Ä‘Ã£ visit
                    if path.contains_node(neighbor):
                        continue
                        
                    # Táº¡o path má»›i
                    new_path = path.copy()
                    
                    # Láº¥y edge info
                    edge_data = self.graph.graph.get_edge_data(current_node, neighbor)
                    relation = edge_data.get('relation', 'unknown') if edge_data else 'unknown'
                    edge_info = (current_node, neighbor, relation)
                    
                    new_path.add_node(neighbor, edge_info)
                    
                    # Score path má»›i
                    new_path.score = self.score_path(new_path)
                    
                    # Kiá»ƒm tra náº¿u Ä‘áº¡t sentence node
                    neighbor_data = graph_data.get(neighbor, {})
                    if neighbor_data.get('type') == 'sentence':
                        completed_paths.append(new_path)
                        print(f"  âœ… Found path to sentence: {neighbor} (score: {new_path.score:.3f})")
                    else:
                        new_candidates.append(new_path)
                        
            # Chá»n top K candidates cho beam tiáº¿p theo
            if new_candidates:
                # Sort by score descending vÃ  chá»n top beam_width
                new_candidates.sort(key=lambda p: p.score, reverse=True)
                beam = new_candidates[:self.beam_width]
                
                # Debug info
                print(f"  ğŸ“ˆ Top scores in beam: {[f'{p.score:.3f}' for p in beam[:5]]}")
            else:
                beam = []
                
        # Combine completed paths vÃ  sort theo score
        all_paths = completed_paths
        all_paths.sort(key=lambda p: p.score, reverse=True)
        
        print(f"\nğŸ‰ Beam Search completed!")
        print(f"  Found {len(completed_paths)} paths to sentences")
        print(f"  Top path score: {all_paths[0].score:.3f}" if all_paths else "  No paths found")
        
        return all_paths
        
    def find_best_paths(self, max_paths: int = 20) -> List[Path]:
        """
        TÃ¬m cÃ¡c path tá»‘t nháº¥t tá»« claim Ä‘áº¿n sentences
        
        Args:
            max_paths: Sá»‘ lÆ°á»£ng paths tá»‘i Ä‘a Ä‘á»ƒ tráº£ vá»
            
        Returns:
            List[Path]: Danh sÃ¡ch paths Ä‘Æ°á»£c sáº¯p xáº¿p theo score
        """
        start_time = time.time()
        
        # Láº¥y claim nodes vÃ  sentence nodes  
        claim_nodes = [node for node, data in self.graph.graph.nodes(data=True) 
                      if data.get('type') == 'claim']
        sentence_nodes = [node for node, data in self.graph.graph.nodes(data=True)
                         if data.get('type') == 'sentence']
                         
        if not claim_nodes:
            print("âš ï¸  No claim nodes found!")
            return []
            
        if not sentence_nodes:
            print("âš ï¸  No sentence nodes found!")
            return []
            
        print(f"ğŸ¯ Found {len(claim_nodes)} claim nodes, {len(sentence_nodes)} sentence nodes")
        
        # Khá»Ÿi táº¡o beam vá»›i paths tá»« má»—i claim node
        current_beam = []
        for claim_node in claim_nodes:
            initial_path = Path([claim_node], [], 0.0)
            current_beam.append(initial_path)
            
        completed_paths = []
        
        # Beam search main loop
        for depth in range(self.max_depth):
            if not current_beam:
                break
                
            next_beam = []
            
            for path in current_beam:
                current_node = path.nodes[-1]
                
                # Kiá»ƒm tra xem node hiá»‡n táº¡i cÃ³ pháº£i sentence khÃ´ng
                current_node_data = self.graph.graph.nodes[current_node]
                if current_node_data.get('type') == 'sentence':
                    # ÄÃ£ Ä‘áº¿n sentence node - cÃ³ thá»ƒ dá»«ng á»Ÿ Ä‘Ã¢y
                    completed_paths.append(path)
                    self.sentence_paths_found += 1
                    continue  # KhÃ´ng expand thÃªm tá»« sentence node
                    
                # Expand path Ä‘áº¿n cÃ¡c neighbors
                for neighbor in self.graph.graph.neighbors(current_node):
                    # TrÃ¡nh cycles
                    if neighbor in path.nodes:
                        continue
                        
                    # Táº¡o path má»›i
                    edge_data = self.graph.graph.get_edge_data(current_node, neighbor, {})
                    edge_label = edge_data.get('label', f"{current_node}->{neighbor}")
                    
                    new_path = Path(
                        path.nodes + [neighbor],
                        path.edges + [edge_label],
                        0.0
                    )
                    
                    # TÃ­nh Ä‘iá»ƒm cho path má»›i
                    new_path.score = self.score_path(new_path)
                    next_beam.append(new_path)
                    self.paths_explored += 1
                    
            # Giá»¯ láº¡i top beam_width paths
            next_beam.sort(key=lambda p: p.score, reverse=True)
            current_beam = next_beam[:self.beam_width]
            
            if self.early_stop_on_sentence and completed_paths:
                break  # Dá»«ng ngay khi tÃ¬m Ä‘Æ°á»£c sentence Ä‘áº§u tiÃªn
            
        # Káº¿t há»£p completed paths vÃ  current beam
        all_paths = completed_paths + current_beam
        
        # Lá»c chá»‰ láº¥y paths káº¿t thÃºc táº¡i sentence nodes
        sentence_paths = []
        for path in all_paths:
            if path.nodes:
                last_node = path.nodes[-1] 
                last_node_data = self.graph.graph.nodes[last_node]
                if last_node_data.get('type') == 'sentence':
                    sentence_paths.append(path)
                    
        # Sáº¯p xáº¿p vÃ  láº¥y top paths
        sentence_paths.sort(key=lambda p: p.score, reverse=True)
        
        end_time = time.time()
        print(f"â±ï¸  Beam search completed in {end_time - start_time:.2f}s")
        print(f"ğŸ“Š Explored {self.paths_explored} paths, found {len(sentence_paths)} sentence paths")
        
        return sentence_paths[:max_paths]
        
    def export_paths_to_file(self, paths: List[Path], output_file: str = None) -> str:
        """
        Export paths ra file JSON Ä‘á»ƒ kháº£o sÃ¡t
        
        Args:
            paths: Danh sÃ¡ch paths cáº§n export
            output_file: ÄÆ°á»ng dáº«n file output (náº¿u None sáº½ tá»± generate)
            
        Returns:
            str: ÄÆ°á»ng dáº«n file Ä‘Ã£ lÆ°u
        """
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # Use absolute path to ensure correct directory
            current_dir = os.getcwd()
            if current_dir.endswith('vncorenlp'):
                # If we're in vncorenlp directory, go back to parent
                current_dir = os.path.dirname(current_dir)
            output_file = os.path.join(current_dir, "output", f"beam_search_paths_{timestamp}.json")
            
        # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # Prepare data for export
        export_data = {
            'search_config': {
                'beam_width': self.beam_width,
                'max_depth': self.max_depth,
                'word_match_weight': self.word_match_weight,
                'entity_bonus': self.entity_bonus,
                'length_penalty': self.length_penalty,
                'sentence_bonus': self.sentence_bonus
            },
            'claim_words': list(self.claim_words),
            'total_paths_found': len(paths),
            'paths': []
        }
        
        # Prepare graph data for node details
        graph_data = dict(self.graph.graph.nodes(data=True))
        
        for i, path in enumerate(paths):
            path_data = path.to_dict()
            
            # ThÃªm thÃ´ng tin chi tiáº¿t vá» nodes
            path_data['node_details'] = []
            for node_id in path.nodes:
                node_info = graph_data.get(node_id, {})
                path_data['node_details'].append({
                    'id': node_id,
                    'type': node_info.get('type', 'unknown'),
                    'text': node_info.get('text', ''),
                    'pos': node_info.get('pos', ''),
                    'lemma': node_info.get('lemma', '')
                })
                
            export_data['paths'].append(path_data)
            
        # Write to file
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ’¾ Exported {len(paths)} paths to: {output_file}")
        return output_file
        
    def export_paths_summary(self, paths: List[Path], output_file: str = None) -> str:
        """
        Export summary dá»… Ä‘á»c cá»§a paths
        
        Args:
            paths: Danh sÃ¡ch paths
            output_file: File output (náº¿u None sáº½ tá»± generate)
            
        Returns:
            str: ÄÆ°á»ng dáº«n file Ä‘Ã£ lÆ°u
        """
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # Use absolute path to ensure correct directory
            current_dir = os.getcwd()
            if current_dir.endswith('vncorenlp'):
                # If we're in vncorenlp directory, go back to parent
                current_dir = os.path.dirname(current_dir)
            output_file = os.path.join(current_dir, "output", f"beam_search_summary_{timestamp}.txt")
            
        # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # Prepare graph data
        graph_data = dict(self.graph.graph.nodes(data=True))
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("ğŸ¯ BEAM SEARCH PATH ANALYSIS\n")
            f.write("="*60 + "\n\n")
            
            f.write(f"Search Configuration:\n")
            f.write(f"  Beam Width: {self.beam_width}\n")
            f.write(f"  Max Depth: {self.max_depth}\n")
            f.write(f"  Claim Words: {', '.join(self.claim_words)}\n")
            f.write(f"  Total Paths Found: {len(paths)}\n\n")
            
            for i, path in enumerate(paths[:10]):  # Top 10 paths
                f.write(f"PATH #{i+1} (Score: {path.score:.3f})\n")
                f.write("-" * 40 + "\n")
                
                f.write(f"Length: {len(path.nodes)} nodes\n")
                f.write(f"Word Matches: {len(path.word_matches) if hasattr(path, 'word_matches') else 'None'}\n")
                f.write(f"Entities Visited: {', '.join(path.entities_visited) if path.entities_visited else 'None'}\n")
                f.write(f"Path Type: {path._get_path_summary()}\n\n")
                
                f.write("Detailed Path:\n")
                for j, node_id in enumerate(path.nodes):
                    node_info = graph_data.get(node_id, {})
                    node_type = node_info.get('type', 'unknown').upper()
                    node_text = node_info.get('text', '')[:50]  # Truncate long text
                    
                    prefix = "  START: " if j == 0 else f"  {j:2d}: "
                    f.write(f"{prefix}[{node_type}] {node_text}\n")
                    
                    if j < len(path.edges):
                        edge_info = path.edges[j]
                        f.write(f"       â””â”€ ({edge_info[2]}) â”€>\n")
                        
                f.write("\n" + "="*60 + "\n\n")
                
        print(f"ğŸ“„ Exported paths summary to: {output_file}")
        return output_file

    def multi_level_beam_search(
        self,
        max_levels: int = 3,
        beam_width_per_level: int = 3,
        min_new_sentences: int = 2,   # â¶ báº£o Ä‘áº£m má»—i level cÃ³ â‰¥ 2 cÃ¢u má»›i
        advanced_data_filter=None,
        claim_text: str = "",
        entities=None,
        filter_top_k: int = 2
    ) -> Dict[int, List[Path]]:
        """
        Multi-level beam search: tá»« claim â†’ sentences â†’ sentences liÃªn quan â†’ ...
        
        Args:
            max_levels: Sá»‘ levels tá»‘i Ä‘a (k)
            beam_width_per_level: Sá»‘ sentences giá»¯ láº¡i má»—i level
            
        Returns:
            Dict[level, List[Path]]: Sentences theo tá»«ng level
        """
        results = {}
        all_found_sentences = set()  # Track sentences Ä‘Ã£ tÃ¬m Ä‘á»ƒ trÃ¡nh trÃ¹ng
        
        print(f"ğŸ¯ Starting Multi-Level Beam Search (max_levels={max_levels}, beam_width={beam_width_per_level})")
        
        # Level 0: Beam search tá»« claim
        print(f"\nğŸ“ LEVEL 0: Claim â†’ Sentences")
        level_0_paths = self.find_best_paths(max_paths=beam_width_per_level)
        level_0_sentences = self._extract_sentence_nodes_from_paths(level_0_paths)
        
        results[0] = level_0_paths
        all_found_sentences.update(level_0_sentences)
        
        print(f"   Found {len(level_0_sentences)} sentences at level 0")
        
        # Levels 1 to k: Beam search tá»« sentences cá»§a level trÆ°á»›c
        current_sentence_nodes = level_0_sentences
        
        for level in range(1, max_levels + 1):
            if not current_sentence_nodes:
                print(f"   No sentences to expand from level {level-1}")
                break
                
            print(f"\nğŸ“ LEVEL {level}: Sentences â†’ New Sentences")
            level_paths = []
            new_sentence_nodes = set()
            
            # Beam search tá»« má»—i sentence cá»§a level trÆ°á»›c
            for sentence_node in current_sentence_nodes:
                print(f"   Expanding from sentence: {sentence_node}")
                
                # Beam search tá»« sentence nÃ y
                sentence_paths = self._beam_search_from_sentence(
                    sentence_node, 
                    max_paths=beam_width_per_level,
                    exclude_sentences=all_found_sentences
                )
                
                # Láº¥y sentences má»›i
                new_sentences = self._extract_sentence_nodes_from_paths(sentence_paths)
                new_sentences = [s for s in new_sentences if s not in all_found_sentences]
                
                level_paths.extend(sentence_paths)
                new_sentence_nodes.update(new_sentences)
                
                print(f"     â†’ Found {len(new_sentences)} new sentences")
            
            # Giá»¯ láº¡i top beam_width_per_level sentences tá»‘t nháº¥t cho level nÃ y
            if level_paths:
                level_paths.sort(key=lambda p: p.score, reverse=True)
                level_paths = level_paths[:beam_width_per_level]

                # â· Láº¥y cÃ¢u má»›i, loáº¡i trÃ¹ng
                final_new_sentences = self._extract_sentence_nodes_from_paths(level_paths)
                unique_new = [s for s in final_new_sentences if s not in all_found_sentences]

                # ğŸ”„ Ãp dá»¥ng AdvancedDataFilter (náº¿u cÃ³) Ä‘á»ƒ chá»n seed cho level káº¿ tiáº¿p
                if advanced_data_filter and claim_text and unique_new:
                    try:
                        raw_sentences = [
                            {"sentence": self.graph.graph.nodes[node]["text"]}
                            for node in unique_new
                        ]
                        filtered = advanced_data_filter.multi_stage_filtering_pipeline(
                            sentences=raw_sentences,
                            claim_text=claim_text,
                            entities=entities or [],
                            max_final_sentences=filter_top_k,
                            min_quality_score=0.25,
                            min_relevance_score=0.2
                        )["filtered_sentences"]

                        # Láº¥y node-ids tÆ°Æ¡ng á»©ng vá»›i cÃ¡c cÃ¢u cÃ²n láº¡i sau lá»c
                        filtered_texts = {s["sentence"] for s in filtered}
                        filtered_nodes = [
                            n for n in unique_new
                            if self.graph.graph.nodes[n]["text"] in filtered_texts
                        ]
                        if filtered_nodes:
                            unique_new = filtered_nodes[:filter_top_k]
                            # print(f"   ğŸ” Advanced filter giá»¯ {len(unique_new)} cÃ¢u cho level tiáº¿p theo")
                    except Exception as e:
                        print(f"âš ï¸  Advanced filter error (level {level}): {e}")

                # â¸ Náº¿u chÆ°a Ä‘á»§, láº¥y thÃªm cÃ¢u (khÃ´ng trÃ¹ng) tá»« danh sÃ¡ch level_paths (Ä‘Ã£ xáº¿p háº¡ng)
                if len(unique_new) < min_new_sentences:
                    for path in level_paths:
                        for node in path.nodes[::-1]:  # duyá»‡t tá»« cuá»‘i path
                            node_data = self.graph.graph.nodes[node]
                            if node_data.get('type') == 'sentence' and node not in all_found_sentences:
                                unique_new.append(node)
                                if len(unique_new) >= min_new_sentences:
                                    break
                        if len(unique_new) >= min_new_sentences:
                            break

                # â¹ Cáº­p nháº­t káº¿t quáº£ / tracking
                results[level] = level_paths
                all_found_sentences.update(unique_new)
                current_sentence_nodes = unique_new
                
                print(f"   Level {level} final: {len(unique_new)} sentences")
            else:
                print(f"   Level {level}: No new sentences found")
                break
        
        print(f"\nğŸ‰ Multi-Level Search completed! Total levels: {len(results)}")
        return results

    def _extract_sentence_nodes_from_paths(self, paths: List[Path]) -> List[str]:
        """Extract unique sentence node IDs tá»« paths"""
        sentence_nodes = set()
        for path in paths:
            for node in path.nodes:
                node_data = self.graph.graph.nodes.get(node, {})
                if node_data.get('type') == 'sentence':
                    sentence_nodes.add(node)
        return list(sentence_nodes)

    def _beam_search_from_sentence(self, start_sentence: str, max_paths: int = 3, exclude_sentences: Set[str] = None) -> List[Path]:
        """
        Beam search tá»« má»™t sentence node Ä‘á»ƒ tÃ¬m sentences liÃªn quan
        
        Args:
            start_sentence: Sentence node Ä‘á»ƒ báº¯t Ä‘áº§u
            max_paths: Sá»‘ paths tá»‘i Ä‘a
            exclude_sentences: Sentences cáº§n loáº¡i trá»« (Ä‘Ã£ tÃ¬m trÆ°á»›c Ä‘Ã³)
        """
        if exclude_sentences is None:
            exclude_sentences = set()
        
        # Initialize beam tá»« sentence node
        beam = [Path([start_sentence])]
        completed_paths = []
        
        # Reduced depth for sentence-to-sentence search
        max_depth = min(self.max_depth // 2, 15)  # Shorter paths for efficiency
        
        for depth in range(max_depth):
            if not beam:
                break
                
            new_candidates = []
            
            for path in beam:
                current_node = path.get_current_node()
                
                # Expand to neighbors
                for neighbor in self.graph.graph.neighbors(current_node):
                    # Avoid cycles
                    if path.contains_node(neighbor):
                        continue
                    
                    # Táº¡o path má»›i
                    new_path = path.copy()
                    edge_data = self.graph.graph.get_edge_data(current_node, neighbor)
                    relation = edge_data.get('relation', 'unknown') if edge_data else 'unknown'
                    edge_info = (current_node, neighbor, relation)
                    
                    new_path.add_node(neighbor, edge_info)
                    new_path.score = self.score_path(new_path)
                    
                    # Check if reached new sentence
                    neighbor_data = self.graph.graph.nodes.get(neighbor, {})
                    if (neighbor_data.get('type') == 'sentence' and 
                        neighbor != start_sentence and  # Not same as start
                        neighbor not in exclude_sentences):  # Not already found
                        completed_paths.append(new_path)
                    else:
                        new_candidates.append(new_path)
            
            # Keep top candidates
            if new_candidates:
                new_candidates.sort(key=lambda p: p.score, reverse=True)
                beam = new_candidates[:self.beam_width]
            else:
                beam = []
        
        # Return top sentence paths
        completed_paths.sort(key=lambda p: p.score, reverse=True)
        return completed_paths[:max_paths] 