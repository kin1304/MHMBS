#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
MINT TextGraph - Beam Search Path Finding
T√¨m ƒë∆∞·ªùng ƒëi t·ª´ claim ƒë·∫øn sentence nodes b·∫±ng Beam Search
"""

import json
import os
from collections import defaultdict
from typing import List, Dict, Tuple, Set, Optional
import heapq
from datetime import datetime
import time
from difflib import SequenceMatcher
import networkx as nx


class Path:
    """ƒê·∫°i di·ªán cho m·ªôt ƒë∆∞·ªùng ƒëi trong ƒë·ªì th·ªã"""
    
    def __init__(self, nodes: List[str], edges: Optional[List[Tuple[str, str, str]]] = None, score: float = 0.0):
        self.nodes = nodes  # Danh s√°ch node IDs
        self.edges = edges or []  # Danh s√°ch (from_node, to_node, relation)
        self.score = score  # ƒêi·ªÉm ƒë√°nh gi√° path
        self.claim_words = set()  # Words trong claim ƒë·ªÉ so s√°nh
        self.word_matches = set()  # ‚úÖ TH√äM: Set of matched words
        self.path_words = set()   # T·ª´ trong path
        self.entities_visited = set()  # Entities ƒë√£ ƒëi qua
        
    def __lt__(self, other):
        """So s√°nh ƒë·ªÉ sort paths theo score"""
        return self.score < other.score
        
    def add_node(self, node_id: str, edge_info: Optional[Tuple[str, str, str]] = None):
        """Th√™m node v√†o path"""
        self.nodes.append(node_id)
        if edge_info:
            self.edges.append(edge_info)
            
    def copy(self):
        """T·∫°o b·∫£n copy c·ªßa path"""
        new_path = Path(self.nodes.copy(), self.edges.copy(), self.score)
        new_path.claim_words = self.claim_words.copy()
        new_path.word_matches = self.word_matches.copy()
        new_path.path_words = self.path_words.copy()
        new_path.entities_visited = self.entities_visited.copy()
        return new_path
        
    def get_current_node(self):
        """L·∫•y node hi·ªán t·∫°i (cu·ªëi path)"""
        return self.nodes[-1] if self.nodes else None
        
    def contains_node(self, node_id: str):
        """Ki·ªÉm tra path c√≥ ch·ª©a node n√†y kh√¥ng"""
        return node_id in self.nodes
        
    def to_dict(self):
        """Convert path th√†nh dictionary ƒë·ªÉ export"""
        return {
            'nodes': self.nodes,
            'edges': self.edges,
            'score': self.score,
            'length': len(self.nodes),
            'claim_words_matched': len(self.claim_words.intersection(self.path_words)),
            'total_claim_words': len(self.claim_words),
            'entities_visited': list(self.entities_visited),
            'path_summary': self._get_path_summary()
        }
        
    def _get_path_summary(self):
        """T·∫°o summary ng·∫Øn g·ªçn c·ªßa path"""
        node_types = []
        for node in self.nodes:
            if node.startswith('claim'):
                node_types.append('CLAIM')
            elif node.startswith('word'):
                node_types.append('WORD')
            elif node.startswith('sentence'):
                node_types.append('SENTENCE')
            elif node.startswith('entity'):
                node_types.append('ENTITY')
            else:
                node_types.append('UNKNOWN')
        return ' -> '.join(node_types)


class BeamSearchPathFinder:
    """Beam Search ƒë·ªÉ t√¨m ƒë∆∞·ªùng ƒëi t·ª´ claim ƒë·∫øn sentence nodes"""
    
    def __init__(self, text_graph, beam_width: int = 25, max_depth: int = 30, allow_skip_edge: bool = False):
        self.graph = text_graph
        self.beam_width = beam_width
        self.max_depth = max_depth
        # Cho ph√©p "nh·∫£y" qua m·ªôt n√∫t trung gian (2-hop) n·∫øu c·∫ßn m·ªü r·ªông ƒëa d·∫°ng
        self.allow_skip_edge = allow_skip_edge
        self.claim_words = set()  # Words trong claim
        
        # Scoring weights - ‚úÖ C·∫¢I THI·ªÜN WEIGHTS
        self.word_match_weight = 5.0        # TƒÉng t·ª´ 3.0 l√™n 5.0
        self.semantic_match_weight = 3.0    # ‚úÖ M·ªöI: Semantic similarity
        self.entity_bonus = 2.5             # TƒÉng t·ª´ 2.0 l√™n 2.5
        self.length_penalty = 0.05          # Gi·∫£m t·ª´ 0.1 xu·ªëng 0.05
        self.sentence_bonus = 4.0           
        self.fuzzy_match_weight = 2.0       # ‚úÖ M·ªöI: Fuzzy string matching
        
        # Stats
        self.paths_explored = 0
        self.sentence_paths_found = 0
        
        # New flag
        self.early_stop_on_sentence = True
        
    def extract_claim_words(self):
        """Tr√≠ch xu·∫•t t·∫•t c·∫£ t·ª´ trong claim ƒë·ªÉ so s√°nh"""
        claim_words = set()
        
        if self.graph.claim_node:
            # L·∫•y t·∫•t c·∫£ word nodes connected ƒë·∫øn claim
            for neighbor in self.graph.graph.neighbors(self.graph.claim_node):
                node_data = self.graph.graph.nodes[neighbor]
                if node_data.get('type') == 'word':
                    claim_words.add(node_data.get('text', '').lower())
                    
        self.claim_words = claim_words
        return claim_words
        
    def _calculate_semantic_similarity(self, claim_words, path_words):
        """
        ‚úÖ M·ªöI: T√≠nh semantic similarity gi·ªØa claim v√† path words
        S·ª≠ d·ª•ng Jaccard similarity v√† word overlap
        """
        if not claim_words or not path_words:
            return 0.0
            
        # Jaccard similarity
        intersection = claim_words.intersection(path_words)
        union = claim_words.union(path_words)
        jaccard = len(intersection) / len(union) if union else 0.0
        
        # Word overlap ratio
        overlap_ratio = len(intersection) / len(claim_words) if claim_words else 0.0
        
        # Combine scores
        semantic_score = (jaccard * 0.4) + (overlap_ratio * 0.6)
        return semantic_score
        
    def _calculate_fuzzy_similarity(self, claim_text, sentence_text):
        """
        ‚úÖ M·ªöI: T√≠nh fuzzy string similarity
        """
        if not claim_text or not sentence_text:
            return 0.0
            
        # Normalize texts
        claim_normalized = claim_text.lower().strip()
        sentence_normalized = sentence_text.lower().strip()
        
        # Calculate similarity
        similarity = SequenceMatcher(None, claim_normalized, sentence_normalized).ratio()
        return similarity
        
    def score_path(self, path: Path) -> float:
        """‚úÖ C·∫¢I THI·ªÜN: T√≠nh ƒëi·ªÉm cho m·ªôt path v·ªõi nhi·ªÅu metrics h∆°n"""
        
        if not path.nodes:
            return 0.0
            
        # L·∫•y claim text ƒë·ªÉ so s√°nh
        claim_text = ""
        claim_words = set()
        
        for node in path.nodes:
            node_data = self.graph.graph.nodes[node]
            if node_data.get('type') == 'claim':
                claim_text = node_data.get('text', '')
                claim_words = set(claim_text.lower().split())
                break
                
        # Base score
        score = 0.0
        
        # 1. ‚úÖ C·∫¢I THI·ªÜN: Enhanced Word matching score
        path_words = set()
        sentence_texts = []
        
        for node in path.nodes:
            node_data = self.graph.graph.nodes[node]
            node_text = node_data.get('text', '')
            node_type = node_data.get('type', '')
            
            if node_text:
                path_words.update(node_text.lower().split())
                
                # Collect sentence texts for fuzzy matching
                if node_type == 'sentence':
                    sentence_texts.append(node_text)
                
        if claim_words:
            word_matches = claim_words.intersection(path_words)
            word_match_ratio = len(word_matches) / len(claim_words)
            score += word_match_ratio * self.word_match_weight
            path.word_matches = word_matches
            
            # 2. ‚úÖ M·ªöI: Semantic similarity
            semantic_score = self._calculate_semantic_similarity(claim_words, path_words)
            score += semantic_score * self.semantic_match_weight
            
        # 3. ‚úÖ M·ªöI: Fuzzy matching v·ªõi sentences
        if claim_text and sentence_texts:
            max_fuzzy_score = 0.0
            for sentence_text in sentence_texts:
                fuzzy_score = self._calculate_fuzzy_similarity(claim_text, sentence_text)
                max_fuzzy_score = max(max_fuzzy_score, fuzzy_score)
            score += max_fuzzy_score * self.fuzzy_match_weight
            
        # 4. ‚úÖ ENHANCED: Dual-source entity scoring v·ªõi weighted bonus
        entity_bonus_total = 0.0
        dual_source_entities = 0
        single_source_entities = 0
        
        for node in path.nodes:
            node_data = self.graph.graph.nodes[node]
            if node_data.get('type') == 'entity':
                # Get entity score (dual-source entities have score ‚â• 2.0)
                entity_score = node_data.get('entity_score', 1.0)
                is_dual_source = node_data.get('is_dual_source', False)
                
                # All entities get equal treatment now
                entity_bonus_total += entity_score * self.entity_bonus
                if is_dual_source:
                    dual_source_entities += 1
                else:
                    single_source_entities += 1
                
        score += entity_bonus_total
        
        # Store entity path metadata for debugging
        path.dual_source_count = dual_source_entities
        path.single_source_count = single_source_entities
        # Keep entities_visited as set - don't overwrite with int
        
        # 5. ‚úÖ C·∫¢I THI·ªÜN: Gi·∫£m length penalty
        score -= len(path.nodes) * self.length_penalty
        
        # 6. ‚úÖ TH√äM: Sentence relevance bonus
        sentence_count = sum(1 for node in path.nodes 
                           if self.graph.graph.nodes[node].get('type') == 'sentence')
        if sentence_count > 0:
            score += sentence_count * 1.5  # Bonus cho m·ªói sentence trong path
            
        return score
        
    def beam_search(self, start_node: str = None) -> List[Path]:
        """
        Th·ª±c hi·ªán Beam Search t·ª´ claim node ƒë·∫øn sentence nodes
        
        Returns:
            List[Path]: Danh s√°ch c√°c paths t·ªët nh·∫•t t√¨m ƒë∆∞·ª£c
        """
        if start_node is None:
            start_node = self.graph.claim_node
            
        if not start_node:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y claim node ƒë·ªÉ b·∫Øt ƒë·∫ßu beam search")
            return []
            
        # Extract claim words ƒë·ªÉ scoring
        self.extract_claim_words()
        
        # Prepare graph data for faster lookup
        graph_data = dict(self.graph.graph.nodes(data=True))
        
        # Initialize beam v·ªõi path t·ª´ claim node
        beam = [Path([start_node])]
        completed_paths = []  # Paths ƒë√£ ƒë·∫øn sentence nodes
        
        print(f"üéØ Starting Beam Search from {start_node}")
        print(f"üìä Beam width: {self.beam_width}, Max depth: {self.max_depth}")
        print(f"üí≠ Claim words: {self.claim_words}")
        
        for depth in range(self.max_depth):
            if not beam:
                break
                
            print(f"\nüîç Depth {depth + 1}/{self.max_depth} - Current beam size: {len(beam)}")
            
            new_candidates = []
            
            # Expand m·ªói path trong beam hi·ªán t·∫°i
            for path in beam:
                current_node = path.get_current_node()
                
                # L·∫•y t·∫•t c·∫£ neighbors c·ªßa current node
                neighbors = list(self.graph.graph.neighbors(current_node))
                
                for neighbor in neighbors:
                    # Tr√°nh cycle - kh√¥ng quay l·∫°i node ƒë√£ visit
                    if path.contains_node(neighbor):
                        continue
                        
                    # T·∫°o path m·ªõi
                    new_path = path.copy()
                    
                    # L·∫•y edge info
                    edge_data = self.graph.graph.get_edge_data(current_node, neighbor)
                    relation = edge_data.get('relation', 'unknown') if edge_data else 'unknown'
                    edge_info = (current_node, neighbor, relation)
                    
                    new_path.add_node(neighbor, edge_info)
                    
                    # Score path m·ªõi
                    new_path.score = self.score_path(new_path)
                    
                    # Ki·ªÉm tra n·∫øu ƒë·∫°t sentence node
                    neighbor_data = graph_data.get(neighbor, {})
                    if neighbor_data.get('type') == 'sentence':
                        completed_paths.append(new_path)
                        print(f"  ‚úÖ Found path to sentence: {neighbor} (score: {new_path.score:.3f})")
                    else:
                        new_candidates.append(new_path)
                        
            # Ch·ªçn top K candidates cho beam ti·∫øp theo
            if new_candidates:
                # Sort by score descending v√† ch·ªçn top beam_width
                new_candidates.sort(key=lambda p: p.score, reverse=True)
                beam = new_candidates[:self.beam_width]
                
                # Debug info
                print(f"  üìà Top scores in beam: {[f'{p.score:.3f}' for p in beam[:5]]}")
            else:
                beam = []
                
        # Combine completed paths v√† sort theo score
        all_paths = completed_paths
        all_paths.sort(key=lambda p: p.score, reverse=True)
        
        print(f"\nüéâ Beam Search completed!")
        print(f"  Found {len(completed_paths)} paths to sentences")
        print(f"  Top path score: {all_paths[0].score:.3f}" if all_paths else "  No paths found")
        
        return all_paths
        
    def find_best_paths(self, max_paths: int = 20) -> List[Path]:
        """
        T√¨m c√°c path t·ªët nh·∫•t t·ª´ claim ƒë·∫øn sentences
        
        Args:
            max_paths: S·ªë l∆∞·ª£ng paths t·ªëi ƒëa ƒë·ªÉ tr·∫£ v·ªÅ
            
        Returns:
            List[Path]: Danh s√°ch paths ƒë∆∞·ª£c s·∫Øp x·∫øp theo score
        """
        start_time = time.time()
        
        # L·∫•y claim nodes v√† sentence nodes  
        claim_nodes = [node for node, data in self.graph.graph.nodes(data=True) 
                      if data.get('type') == 'claim']
        sentence_nodes = [node for node, data in self.graph.graph.nodes(data=True)
                         if data.get('type') == 'sentence']
                         
        if not claim_nodes:
            print("‚ö†Ô∏è  No claim nodes found!")
            return []
            
        if not sentence_nodes:
            print("‚ö†Ô∏è  No sentence nodes found!")
            return []
            
        print(f"üéØ Found {len(claim_nodes)} claim nodes, {len(sentence_nodes)} sentence nodes")
        
        # Kh·ªüi t·∫°o beam v·ªõi paths t·ª´ m·ªói claim node
        current_beam = []
        for claim_node in claim_nodes:
            initial_path = Path([claim_node], [], 0.0)
            current_beam.append(initial_path)
            
        completed_paths = []
        
        # Beam search main loop
        for depth in range(self.max_depth):
            if not current_beam:
                break
                
            next_beam = []
            
            for path in current_beam:
                current_node = path.nodes[-1]
                
                # Ki·ªÉm tra xem node hi·ªán t·∫°i c√≥ ph·∫£i sentence kh√¥ng
                current_node_data = self.graph.graph.nodes[current_node]
                if current_node_data.get('type') == 'sentence':
                    # ƒê√£ ƒë·∫øn sentence node - c√≥ th·ªÉ d·ª´ng ·ªü ƒë√¢y
                    completed_paths.append(path)
                    self.sentence_paths_found += 1
                    continue  # Kh√¥ng expand th√™m t·ª´ sentence node
                    
                # Expand path ƒë·∫øn c√°c neighbors
                for neighbor in self.graph.graph.neighbors(current_node):
                    # Tr√°nh cycles
                    if neighbor in path.nodes:
                        continue
                        
                    # T·∫°o path m·ªõi
                    edge_data = self.graph.graph.get_edge_data(current_node, neighbor, {})
                    edge_label = edge_data.get('label', f"{current_node}->{neighbor}")
                    
                    new_path = Path(
                        path.nodes + [neighbor],
                        path.edges + [edge_label],
                        0.0
                    )
                    
                    # T√≠nh ƒëi·ªÉm cho path m·ªõi
                    new_path.score = self.score_path(new_path)
                    next_beam.append(new_path)
                    self.paths_explored += 1
                    
            # Gi·ªØ l·∫°i top beam_width paths
            next_beam.sort(key=lambda p: p.score, reverse=True)
            current_beam = next_beam[:self.beam_width]
            
            if self.early_stop_on_sentence and completed_paths:
                break  # D·ª´ng ngay khi t√¨m ƒë∆∞·ª£c sentence ƒë·∫ßu ti√™n
            
        # K·∫øt h·ª£p completed paths v√† current beam
        all_paths = completed_paths + current_beam
        
        # L·ªçc ch·ªâ l·∫•y paths k·∫øt th√∫c t·∫°i sentence nodes
        sentence_paths = []
        for path in all_paths:
            if path.nodes:
                last_node = path.nodes[-1] 
                last_node_data = self.graph.graph.nodes[last_node]
                if last_node_data.get('type') == 'sentence':
                    sentence_paths.append(path)
                    
        # S·∫Øp x·∫øp v√† l·∫•y top paths
        sentence_paths.sort(key=lambda p: p.score, reverse=True)
        
        end_time = time.time()
        print(f"‚è±Ô∏è  Beam search completed in {end_time - start_time:.2f}s")
        print(f"üìä Explored {self.paths_explored} paths, found {len(sentence_paths)} sentence paths")
        
        return sentence_paths[:max_paths]
        
    def export_paths_to_file(self, paths: List[Path], output_file: str = None) -> str:
        """
        Export paths ra file JSON ƒë·ªÉ kh·∫£o s√°t
        
        Args:
            paths: Danh s√°ch paths c·∫ßn export
            output_file: ƒê∆∞·ªùng d·∫´n file output (n·∫øu None s·∫Ω t·ª± generate)
            
        Returns:
            str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
        """
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # Use absolute path to ensure correct directory
            current_dir = os.getcwd()
            if current_dir.endswith('vncorenlp'):
                # If we're in vncorenlp directory, go back to parent
                current_dir = os.path.dirname(current_dir)
            output_file = os.path.join(current_dir, "output", f"beam_search_paths_{timestamp}.json")
            
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # Prepare data for export
        export_data = {
            'search_config': {
                'beam_width': self.beam_width,
                'max_depth': self.max_depth,
                'word_match_weight': self.word_match_weight,
                'entity_bonus': self.entity_bonus,
                'length_penalty': self.length_penalty,
                'sentence_bonus': self.sentence_bonus
            },
            'claim_words': list(self.claim_words),
            'total_paths_found': len(paths),
            'paths': []
        }
        
        # Prepare graph data for node details
        graph_data = dict(self.graph.graph.nodes(data=True))
        
        for i, path in enumerate(paths):
            path_data = path.to_dict()
            
            # Th√™m th√¥ng tin chi ti·∫øt v·ªÅ nodes
            path_data['node_details'] = []
            for node_id in path.nodes:
                node_info = graph_data.get(node_id, {})
                path_data['node_details'].append({
                    'id': node_id,
                    'type': node_info.get('type', 'unknown'),
                    'text': node_info.get('text', ''),
                    'pos': node_info.get('pos', ''),
                    'lemma': node_info.get('lemma', '')
                })
                
            export_data['paths'].append(path_data)
            
        # Write to file
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
            
        print(f"üíæ Exported {len(paths)} paths to: {output_file}")
        return output_file
        
    def export_paths_summary(self, paths: List[Path], output_file: str = None) -> str:
        """
        Export summary d·ªÖ ƒë·ªçc c·ªßa paths
        
        Args:
            paths: Danh s√°ch paths
            output_file: File output (n·∫øu None s·∫Ω t·ª± generate)
            
        Returns:
            str: ƒê∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
        """
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # Use absolute path to ensure correct directory
            current_dir = os.getcwd()
            if current_dir.endswith('vncorenlp'):
                # If we're in vncorenlp directory, go back to parent
                current_dir = os.path.dirname(current_dir)
            output_file = os.path.join(current_dir, "output", f"beam_search_summary_{timestamp}.txt")
            
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # Prepare graph data
        graph_data = dict(self.graph.graph.nodes(data=True))
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("üéØ BEAM SEARCH PATH ANALYSIS\n")
            f.write("="*60 + "\n\n")
            
            f.write(f"Search Configuration:\n")
            f.write(f"  Beam Width: {self.beam_width}\n")
            f.write(f"  Max Depth: {self.max_depth}\n")
            f.write(f"  Claim Words: {', '.join(self.claim_words)}\n")
            f.write(f"  Total Paths Found: {len(paths)}\n\n")
            
            for i, path in enumerate(paths[:10]):  # Top 10 paths
                f.write(f"PATH #{i+1} (Score: {path.score:.3f})\n")
                f.write("-" * 40 + "\n")
                
                f.write(f"Length: {len(path.nodes)} nodes\n")
                f.write(f"Word Matches: {len(path.word_matches) if hasattr(path, 'word_matches') else 'None'}\n")
                f.write(f"Entities Visited: {', '.join(path.entities_visited) if path.entities_visited else 'None'}\n")
                f.write(f"Path Type: {path._get_path_summary()}\n\n")
                
                f.write("Detailed Path:\n")
                for j, node_id in enumerate(path.nodes):
                    node_info = graph_data.get(node_id, {})
                    node_type = node_info.get('type', 'unknown').upper()
                    node_text = node_info.get('text', '')[:50]  # Truncate long text
                    
                    prefix = "  START: " if j == 0 else f"  {j:2d}: "
                    f.write(f"{prefix}[{node_type}] {node_text}\n")
                    
                    if j < len(path.edges):
                        edge_info = path.edges[j]
                        f.write(f"       ‚îî‚îÄ ({edge_info[2]}) ‚îÄ>\n")
                        
                f.write("\n" + "="*60 + "\n\n")
                
        print(f"üìÑ Exported paths summary to: {output_file}")
        return output_file

    def multi_level_beam_search(
        self,
        max_levels: int = 3,
        beam_width_per_level: int = 3,
        min_new_sentences: int = 2,   # ‚ù∂ b·∫£o ƒë·∫£m m·ªói level c√≥ ‚â• 2 c√¢u m·ªõi
        sbert_model=None,
        claim_text: str = "",
        entities=None,
        filter_top_k: int = 5,
        use_phobert: bool = False  # Use SBERT by default, PhoBERT if True
    ) -> Dict[int, List[Path]]:
        """
        Multi-level beam search: t·ª´ claim ‚Üí sentences ‚Üí sentences li√™n quan ‚Üí ...
        
        Args:
            max_levels: S·ªë levels t·ªëi ƒëa (k)
            beam_width_per_level: S·ªë sentences gi·ªØ l·∫°i m·ªói level
            
        Returns:
            Dict[level, List[Path]]: Sentences theo t·ª´ng level
        """
        results = {}
        all_found_sentences = set()  # Track sentences ƒë√£ t√¨m ƒë·ªÉ tr√°nh tr√πng
        
        print(f"üéØ Starting Multi-Level Beam Search (max_levels={max_levels}, beam_width={beam_width_per_level})")
        
        # Level 0: Beam search t·ª´ claim
        print(f"\nüìç LEVEL 0: Claim ‚Üí Sentences")
        level_0_paths = self.find_best_paths(max_paths=beam_width_per_level)
        level_0_sentences = self._extract_sentence_nodes_from_paths(level_0_paths)
        
        results[0] = level_0_paths
        all_found_sentences.update(level_0_sentences)
        
        print(f"   Found {len(level_0_sentences)} sentences at level 0")
        
        # Levels 1 to k: Beam search t·ª´ sentences c·ªßa level tr∆∞·ªõc
        current_sentence_nodes = level_0_sentences
        
        for level in range(1, max_levels + 1):
            if not current_sentence_nodes:
                print(f"   No sentences to expand from level {level-1}")
                break
                
            print(f"\nüìç LEVEL {level}: Sentences ‚Üí New Sentences")
            level_paths = []
            new_sentence_nodes = set()
            
            # Beam search t·ª´ m·ªói sentence c·ªßa level tr∆∞·ªõc
            for sentence_node in current_sentence_nodes:
                print(f"   Expanding from sentence: {sentence_node}")
                
                # Beam search t·ª´ sentence n√†y
                sentence_paths = self._beam_search_from_sentence(
                    sentence_node, 
                    max_paths=beam_width_per_level,
                    exclude_sentences=all_found_sentences
                )
                
                # L·∫•y sentences m·ªõi
                new_sentences = self._extract_sentence_nodes_from_paths(sentence_paths)
                new_sentences = [s for s in new_sentences if s not in all_found_sentences]
                
                level_paths.extend(sentence_paths)
                new_sentence_nodes.update(new_sentences)
                
                print(f"     ‚Üí Found {len(new_sentences)} new sentences")
            
            # Gi·ªØ l·∫°i top beam_width_per_level sentences t·ªët nh·∫•t cho level n√†y
            if level_paths:
                level_paths.sort(key=lambda p: p.score, reverse=True)
                level_paths = level_paths[:beam_width_per_level]

                # ‚ù∑ L·∫•y c√¢u m·ªõi, lo·∫°i tr√πng
                final_new_sentences = self._extract_sentence_nodes_from_paths(level_paths)
                unique_new = [s for s in final_new_sentences if s not in all_found_sentences]

                # üîç Apply SBERT/PhoBERT filtering at each level
                if sbert_model and claim_text and unique_new:
                    try:
                        # Get sentence texts from nodes
                        sentence_texts = []
                        for node in unique_new:
                            node_text = self.graph.graph.nodes[node].get("text", "")
                            if node_text:
                                sentence_texts.append(node_text)
                        
                        if sentence_texts:
                            # Calculate similarities using SBERT or PhoBERT
                            if use_phobert and hasattr(self.graph, 'get_sentence_similarity'):
                                # Use PhoBERT via TextGraph method
                                print(f"   üîç Using PhoBERT for level {level} filtering...")
                                similarities = []
                                for sent_text in sentence_texts:
                                    try:
                                        sim = self.graph.get_sentence_similarity(sent_text, claim_text)
                                        similarities.append(sim)
                                    except Exception as e:
                                        print(f"   ‚ö†Ô∏è PhoBERT similarity error: {e}")
                                        similarities.append(0.0)  # Fallback score
                            else:
                                # Use SBERT
                                print(f"   üîç Using SBERT for level {level} filtering...")
                                from sklearn.metrics.pairwise import cosine_similarity
                                claim_embedding = sbert_model.encode([claim_text])
                                sentence_embeddings = sbert_model.encode(sentence_texts)
                                similarities = cosine_similarity(claim_embedding, sentence_embeddings)[0]
                            
                            # Filter by similarity threshold (0.4 for level filtering)
                            similarity_threshold = 0.4
                            filtered_nodes = []
                            for i, (node, similarity) in enumerate(zip(unique_new, similarities)):
                                if similarity >= similarity_threshold:
                                    filtered_nodes.append(node)
                            
                            # Keep at least filter_top_k sentences even if below threshold
                            if len(filtered_nodes) < filter_top_k and unique_new:
                                # Sort by similarity and take top filter_top_k
                                node_sim_pairs = list(zip(unique_new, similarities))
                                node_sim_pairs.sort(key=lambda x: x[1], reverse=True)
                                filtered_nodes = [node for node, _ in node_sim_pairs[:filter_top_k]]
                            
                            unique_new = filtered_nodes
                            model_name = "PhoBERT" if use_phobert else "SBERT"
                            print(f"   üîç {model_name} level filtering: {len(unique_new)} sentences retained")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è Level filtering error: {e}")
                        # Continue with all sentences if filtering fails
                else:
                    print(f"   üì¶ Collected {len(unique_new)} raw sentences at level {level} (no level filtering)")

                # ‚ù∏ N·∫øu ch∆∞a ƒë·ªß, l·∫•y th√™m c√¢u (kh√¥ng tr√πng) t·ª´ danh s√°ch level_paths (ƒë√£ x·∫øp h·∫°ng)
                if len(unique_new) < min_new_sentences:
                    for path in level_paths:
                        for node in path.nodes[::-1]:  # duy·ªát t·ª´ cu·ªëi path
                            node_data = self.graph.graph.nodes[node]
                            if node_data.get('type') == 'sentence' and node not in all_found_sentences:
                                unique_new.append(node)
                                if len(unique_new) >= min_new_sentences:
                                    break
                        if len(unique_new) >= min_new_sentences:
                            break

                # ‚ùπ C·∫≠p nh·∫≠t k·∫øt qu·∫£ / tracking
                results[level] = level_paths
                all_found_sentences.update(unique_new)
                current_sentence_nodes = unique_new
                
                print(f"   Level {level} final: {len(unique_new)} sentences")
            else:
                print(f"   Level {level}: No new sentences found")
                break
        
        print(f"\nüéâ Multi-Level Search completed! Total levels: {len(results)}")
        return results

    def _extract_sentence_nodes_from_paths(self, paths: List[Path]) -> List[str]:
        """Extract unique sentence node IDs t·ª´ paths"""
        sentence_nodes = set()
        for path in paths:
            for node in path.nodes:
                node_data = self.graph.graph.nodes.get(node, {})
                if node_data.get('type') == 'sentence':
                    sentence_nodes.add(node)
        return list(sentence_nodes)

    def _beam_search_from_sentence(self, start_sentence: str, max_paths: int = 3, exclude_sentences: Set[str] = None) -> List[Path]:
        """
        Beam search t·ª´ m·ªôt sentence node ƒë·ªÉ t√¨m sentences li√™n quan
        
        Args:
            start_sentence: Sentence node ƒë·ªÉ b·∫Øt ƒë·∫ßu
            max_paths: S·ªë paths t·ªëi ƒëa
            exclude_sentences: Sentences c·∫ßn lo·∫°i tr·ª´ (ƒë√£ t√¨m tr∆∞·ªõc ƒë√≥)
        """
        if exclude_sentences is None:
            exclude_sentences = set()
        
        # Initialize beam t·ª´ sentence node
        beam = [Path([start_sentence])]
        completed_paths = []
        
        # Reduced depth for sentence-to-sentence search
        max_depth = min(self.max_depth // 2, 15)  # Shorter paths for efficiency
        
        for depth in range(max_depth):
            if not beam:
                break
                
            new_candidates = []
            
            for path in beam:
                current_node = path.get_current_node()
                
                # Expand to neighbors
                for neighbor in self.graph.graph.neighbors(current_node):
                    # Avoid cycles
                    if path.contains_node(neighbor):
                        continue
                    
                    # T·∫°o path m·ªõi
                    new_path = path.copy()
                    edge_data = self.graph.graph.get_edge_data(current_node, neighbor)
                    relation = edge_data.get('relation', 'unknown') if edge_data else 'unknown'
                    edge_info = (current_node, neighbor, relation)
                    
                    new_path.add_node(neighbor, edge_info)
                    new_path.score = self.score_path(new_path)
                    
                    # Check if reached new sentence
                    neighbor_data = self.graph.graph.nodes.get(neighbor, {})
                    if (neighbor_data.get('type') == 'sentence' and 
                        neighbor != start_sentence and  # Not same as start
                        neighbor not in exclude_sentences):  # Not already found
                        completed_paths.append(new_path)
                    else:
                        new_candidates.append(new_path)
            
            # Keep top candidates
            if new_candidates:
                new_candidates.sort(key=lambda p: p.score, reverse=True)
                beam = new_candidates[:self.beam_width]
            else:
                beam = []
        
        # Return top sentence paths
        completed_paths.sort(key=lambda p: p.score, reverse=True)
        return completed_paths[:max_paths] 